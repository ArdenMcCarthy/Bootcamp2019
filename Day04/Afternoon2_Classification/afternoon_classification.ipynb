{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification -- Images & Hands-On\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "    <li>Processing of complicated data like images</li>\n",
    "    <li>Thinking about models to use for image classification</li>\n",
    "    <li>Implementation of common models</li>\n",
    "    <li>Convolutional neural networks -- an ML greatest hit</li>\n",
    "</ol>\n",
    "\n",
    "## 1. Processing of complicated data like images\n",
    "\n",
    "#### Suppose we begin with colored 32 x 32 pixel images of objects we wish to classify.\n",
    "\n",
    "![](cifar.png)\n",
    "<span style=\"font-size:0.75em;\">CIFAR-10 Krizhevsky et al.</span>\n",
    "\n",
    "### How can we encode the information from one image?\n",
    "![](corgis.png)\n",
    "![](doge.png)\n",
    "<span style=\"font-size:0.75em;\">commonlounge.com; subsubroutine.com</span>\n",
    "\n",
    "### Let's start with a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "(X, y), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f5bf8c470>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[1000], cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As long as each data point is of the same shape, we can unroll these 2- or 3-tensors into long vectors\n",
    "- How many dimensions in each CIFAR-10 data point? Remember this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Flatten each X flatten product.\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classification output will be a vector of 0s except for the target class, which should be a 1.\n",
    "### Presently our output is instead encoded as a single ordinal variable between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_ord = y # In case we need this later\n",
    "\n",
    "\n",
    "\n",
    "# Encode each element of y as 10-length \"one-hot vector\" with binary elements\n",
    "y = keras.utils.to_categorical(y)\n",
    "print(y.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, let's choose an error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true, pred):\n",
    "    # some accuracy metric\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if np.sum(np.dot(true[i], pred[i])) ==1.0:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thinking about models to use for image classification\n",
    "\n",
    "### k-Nearest neighbors\n",
    "\n",
    "* 1-Nearest Neighbors (aka nearest neighbors)\n",
    "    - Use some distance metric to compare each 784-D vector to all training examples\n",
    "    - Order samples by distance\n",
    "    - Classify the same as the smallest distance example\n",
    "* k-Nearest Neighbors\n",
    "    - Classify by committee based on several small distances\n",
    "* Where do these fail?\n",
    "    - This can be bad for big data.  For example, with 60,000 data points, you would need to look at all 60,000 neighbors.\n",
    "* How do these scale with training examples?\n",
    "\n",
    "### Logistic regression\n",
    "* Optimal parameters attained from maximizing the likelihood of dataset, aka minimizing the negative log-likelihood\n",
    "$$\\mathcal{L}(\\theta = \\{W,b\\},\\mathcal{D}) = \\sum_{i=0}^{|\\mathcal{D}|} log(P(Y = y^{(i)} | x^{(i)},W,b))$$\n",
    "![](log_reg.png)\n",
    "* \"nonlinear\" -- though always depends directly on weighted sums of pixels\n",
    "* understanding when your model is wrong is important.  So, these probabilities are very useful.\n",
    "\n",
    "### Random forest classifiers\n",
    "* \"Split\" predictions based on pixels or collection of pixels\n",
    "* Truly nonlinear\n",
    "* This should be looked up: these are very effective for some problems.  Split a good prediction based on the pixles coming in.\n",
    "\n",
    "### Feed-forward neural networks\n",
    "* Nonlinear\n",
    "* Permits \"communication\" (sets of interaction of nonlinearity) between pixels via dense layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of common models\n",
    "### WAIT what haven't I done yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDAITION FOR DATA! Don't want to validate on a test set, because this gives midleading and untrue accuracy.\n",
    "#Train val split before model choosing.\n",
    "\n",
    "X_train = X[:54000]\n",
    "y_train = y[:54000]\n",
    "y_ord_train = y_ord[:54000]\n",
    "\n",
    "X_val = X[54000:]\n",
    "y_val = y[54000:]\n",
    "yord_val = y_ord[54000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "#problem: don't know actual error, runtime is too long for all of the data.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 5)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val[:100])\n",
    "print(score(y_val[:100], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 40 epochs took 25 seconds\n",
      "convergence after 41 epochs took 26 seconds\n",
      "convergence after 43 epochs took 28 seconds\n",
      "convergence after 53 epochs took 33 seconds\n",
      "convergence after 46 epochs took 29 seconds\n",
      "convergence after 48 epochs took 31 seconds\n",
      "convergence after 47 epochs took 30 seconds\n",
      "convergence after 54 epochs took 36 seconds\n",
      "convergence after 46 epochs took 27 seconds\n",
      "convergence after 53 epochs took 31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver = 'saga', tol = 0.01, n_jobs = 4, verbose = 1)\n",
    "model.fit(X_train, y_ord_train)\n",
    "preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent accuracy score for ordinal outputs\n",
    "def score_ord(true, preds):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == preds[i]:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_ord_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-39870c742c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_ord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_ord_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_ord_val' is not defined"
     ]
    }
   ],
   "source": [
    "print(score_ord(y_ord_val.preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifiers - you try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'preds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4209af07934b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'preds'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val.preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,20))\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](conv_net.png)\n",
    "<span style=\"font-size:0.75em;\">easy-tensorflow.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generally, a <i>filter</i> is a rectangular $k \\times l$ matrix of weights.\n",
    "* A single <i>filter</i> traverses an image, elementwise multiplying pixels in its range, adding, and performing a nonlinearity.\n",
    "* The new <i>feature map</i> generated is typically the same size or smaller than the input.\n",
    "* Many <i>feature maps</i> are generated with different <i>filters</i>, each with different weights\n",
    "* <i>Pooling layers</i> serve to reduce feature map dimensionality.\n",
    "* The CNN concludes with generic Dense layers\n",
    "\n",
    "### Examines local areas of photographs -- takes full photo matrix as input, not flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose I have validated the following hyperparameters such that I believe they are optimal. <i>Now</i> we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose train and test, add an additional 1 rank to indicate\n",
    "# greyscale for special layers\n",
    "\n",
    "X_train = X.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "24960/60000 [===========>..................] - ETA: 4:12 - loss: 0.4717 - acc: 0.8557"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-660a593834f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](KaggleMNIST.png)\n",
    "<span style=\"font-size:0.75em;\">Kaggle - Chris Deotte 2018</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9747\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "final = np.zeros_like(preds)\n",
    "final[np.arange(len(preds)), preds.argmax(1)] = 1\n",
    "\n",
    "print(score(y_test, final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nice_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on inductive bias and domain knowledge\n",
    "* CNNs take advantage of our understanding of local features in mapping images to semantic meaning (number labels, dog/cat/plane)\n",
    "* \"A universal function approximator\": The infinitely large dense NN can fit any analytic function exactly with enough data.\n",
    "    - \"Not really\": We rarely have \"enough data\" and can't train infinitely large NNs\n",
    "    - The name of the game is making the network size and data requirements <i>practical</i>\n",
    "* The state of the art usually comes from understanding your problem first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
